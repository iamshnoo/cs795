
Original BBQ data: **bbq_data/english**
Translations of BBQ (using Tower-Plus-9B) model:
- Chinese (simplified): **bbq_data/chinese_simplified**
- French: **bbq_data/french**
- German: **bbq_data/german**
- Hindi: **bbq_data/hindi**

Next, we used Qwen3 4B thinking model to evaluate BBQ data in all languages.
The BBQ examples are then filtered into subsets based on model performance
(correct_all, wrong_all) across languages.
- Correct_all: examples that the model answered correctly in all languages.
- Wrong_all: examples that the model answered incorrectly in all languages.

The SFT datasets use all the examples from the correct_all subset.
The GRPO datasets are created from the SFT datasets by adding rejected responses
generated by GPT-4o-mini to form a preference dataset.

Tasks (done):
- Prepare source data: BBQ + translations
- Evaluate performance of Qwen3 4B thinking model on all languages
- Filter examples into correct_all and wrong_all subsets
- Create SFT datasets from correct_all subset
- Create GRPO datasets from SFT datasets with GPT-4o-mini rejected responses
- SFT of Qwen3 4B thinking model on SFT datasets (5 languages -> 5 fine-tuned
  models)
- GRPO of the SFT fine-tuned models on GRPO datasets (5 languages -> 5 GRPO models)
- Merge LoRA weights of SFT and GRPO models
- Inference script to compare baseline, SFT, and GRPO models
--------------------------------------------------------------------------------

Requirements:
```bash
python 3.11
cuda 12.4
torch 2.6
flash attention 2.7.3
transformers
datasets
wandb
trl
peft
bitsandbytes
liger-kernel
rich
unbabel-comet
```

Commands:
```bash
# SFT Training:
python sft.py #(manually adjust language inside the script)

# GRPO Training:
python grpo.py --lang english/hindi/french/german/chinese_simplified

# LoRA Merging:
python merge_lora.py #(manually adjust mode sft/grpo and language inside the script)

# Inference:
python inference.py --mode baseline/sft/grpo --num_think_tokens 512/1024/2048 --batch_size 8/16
python combine_results.py --results_root /scratch/amukher6/cs795/results/ --output /scratch/amukher6/cs795/results/combined_results.csv

# Translation QE Evaluation:
python translation_qe.py #(manually adjust lang = "zh"  # ["zh", "hi", "fr", "de"])
```

--------------------------------------------------------------------------------

Reference implementations of SFT/GRPO used for hyperparameter tuning:
- https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb#scrollTo=MNfRlfIGKSHI
- https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb#scrollTo=ZT1JfiiTGExB
- https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers
- https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune#fine-tuning-qwen3-with-unsloth
- https://huggingface.co/docs/trl/example_overview


https://discuss.huggingface.co/t/flashattention-2s-16-bit-requirement/67069
- using bnb with flash attention 2 requires casting to bf16 repeateadly, making it slower. So, we disable this as the model fits in memory without quantization.

https://stackoverflow.com/a/78251447
- check all-linear modules in a model for LoRA target modules

```python
import torch
from transformers import Conv1D

def get_specific_layer_names(model):
    # Create a list to store the layer names
    layer_names = []

    # Recursively visit all modules and submodules
    for name, module in model.named_modules():
        # Check if the module is an instance of the specified layers
        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):
            # model name parsing

            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])

    # Remove empty strings and 'lm-head' from the list
    layer_names = [name for name in layer_names if name and name != 'lm-head']
    return layer_names
```

https://huggingface.co/docs/peft/developer_guides/lora#merge-lora-weights-into-the-base-model
https://github.com/huggingface/peft/issues/2264#issuecomment-2523371874
- Merging LoRA weights into the base model (different strategies)


# Results Instruction translation QE:
 en -> zh (0.8170591963160543)
 en -> hi (0.8323525959859535)
 en -> fr (0.8526536049272896)
 en -> de (0.8371817441719412)
